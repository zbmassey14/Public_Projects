{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Starting Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "baseball = pd.read_csv(\"c:/users/zakma/Documents/MS Econ Analytics/MS-Economic-Analytics/SportsBetting/Baseball/baseball_completed.csv\")\n",
    "baseball.drop([\"Pos Summary\", \"Name-additional\", \"Rk\", \"szn\", \"Tm\", \"Lg\"], axis=1, inplace=True)\n",
    "\n",
    "#Clean the data\n",
    "#Drop values that don't meet critera & duplicates\n",
    "baseball = baseball.drop_duplicates()\n",
    "\n",
    "#Dropping NA values & Name columns\n",
    "baseball = baseball.dropna()\n",
    "baseball.drop([\"Name\"], axis = 1, inplace=True)\n",
    "baseball.drop([\"TB\"], axis = 1, inplace=True)\n",
    "\n",
    "#We could create a new target variable\n",
    "baseball[\"runs_per_game\"] = baseball.R/baseball.G\n",
    "\n",
    "#Shuffle the dataset\n",
    "baseball = baseball.sample(frac=1)\n",
    "\n",
    "#View the data\n",
    "baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can split it into test/train/validation\n",
    "random.seed(2112)\n",
    "train, test = train_test_split(baseball, test_size=0.15)\n",
    "test, validation = train_test_split(test, test_size=0.10)\n",
    "#Train = 5378 \n",
    "#Test = 855 \n",
    "#Validation = 95 \n",
    "\n",
    "#Look for outliers in the training data\n",
    "#Probably should drop the outliers\n",
    "sns.boxplot(train.runs_per_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the outliers\n",
    "\n",
    "#To verify there are outliers according to the zscore\n",
    "train[\"z\"] = np.abs(stats.zscore(train.runs_per_game))\n",
    "\n",
    "#We can see all the values that fall outside the z score range\n",
    "#This is good because most of them only have 1-3 games played, which is not the best sample\n",
    "train[train.z > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the vales above\n",
    "train = train[train.z < 3]\n",
    "train = pd.DataFrame(train)\n",
    "\n",
    "\n",
    "#Drop the Runs and Zscore columns\n",
    "train.drop([\"z\", \"R\"], axis = 1, inplace=True)\n",
    "test.drop([\"R\"], axis = 1, inplace=True)\n",
    "validation.drop([\"R\"], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "#We can also set a minimum number of games played for each player\n",
    "#Because a small amount of games could skew the runs/game target\n",
    "#There are 162 games are played over a 6-month season\n",
    "#Requirement to play at least 20% of the season\n",
    "\n",
    "\n",
    "#Drop all obs who played less than 20 games\n",
    "train = train[train[\"G\"]> 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = train.hist()\n",
    "fig1 = fig1.figure\n",
    "fig1.set_size_inches(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the ditributions\n",
    "train.hist()\n",
    "train.hist([\"PA\", \"AB\", \"H\", \"BB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test if data has a normal distribution\n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "result_jb = (jarque_bera(train['runs_per_game']))\n",
    "result_sh = (shapiro(train['runs_per_game']))\n",
    "print(result_jb)\n",
    "print(result_sh)\n",
    "\n",
    "#Null is that there is no difference from normally distributed data\n",
    "#We can reject that based on the pval\n",
    "#Conclude that the data is not normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "\n",
    "#For out first model - KNN it is not necessary to transform the data in any way, because it is a non parametic model\n",
    "#But for the subsquent models we would want to transform the data. \n",
    "#So we are just gonna do it now so we can keep the labels\n",
    "#Since we are going to do this all in the same script\n",
    "\n",
    "#Pull the training data mean/std/min/max\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "train_min = train.min()\n",
    "train_max = train.max()\n",
    "\n",
    "\n",
    "#Normalize the train/testing/validation based off the training features\n",
    "train2 = ((train - train_min)/(train_max-train_min))+1\n",
    "test2 = ((test - train_min)/(train_max-train_min))+1\n",
    "validation2 = ((validation - train_min)/(train_max-train_min))+1\n",
    "\n",
    "\n",
    "#Using the log transofmation\n",
    "train2 = pd.DataFrame((np.log(train2)))\n",
    "test2 = pd.DataFrame((np.log(test2)))\n",
    "validation2 = pd.DataFrame((np.log(validation2)))\n",
    "\n",
    "\n",
    "#View new distributions after the transformation\n",
    "train2.columns = train.columns\n",
    "train2.hist()\n",
    "\n",
    "\n",
    "#Rename the columns\n",
    "test2.columns = test.columns\n",
    "validation2.columns = validation.columns\n",
    "\n",
    "\n",
    "#Need to define the labels\n",
    "#Define inputs/target\n",
    "x_train = train2.drop(\"runs_per_game\", axis = 1).values\n",
    "y_train = train2[\"runs_per_game\"].values\n",
    "x_test= test2.drop(\"runs_per_game\", axis = 1).values\n",
    "y_test = test2[\"runs_per_game\"].values\n",
    "x_val= validation2.drop(\"runs_per_game\", axis = 1).values\n",
    "y_val = validation2[\"runs_per_game\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling\n",
    "\n",
    "#KKN model 1\n",
    "#Running a for loop to pass all the Ks defined in our range\n",
    "#Recording the score so we can visualize what K is best\n",
    "#Almost like gridsearch, but is in a for loop since it is only one feature\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "k_range = range(1,51)\n",
    "scores = {}\n",
    "scores_list = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    scores[k] = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    scores_list.append(metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "#Plotting the results\n",
    "plt.plot(k_range, scores_list)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for optimal K based on a different metric (R2)\n",
    "\n",
    "#KNN Model 2\n",
    "k_range = range(1,51)\n",
    "scores = {}\n",
    "scores_list = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    scores[k] = metrics.r2_score(y_test, y_pred)\n",
    "    scores_list.append(metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "plt.plot(k_range, scores_list)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see K = 10 got the lowest R2/MAE on the testing data\n",
    "#Fit on the training data\n",
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "#Make predictions on the validation dataset using the model above\n",
    "y_pred = knn.predict(x_val)\n",
    "\n",
    "#Evaulate the models performance via MSE\n",
    "def mse(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.square(np.subtract(actual,pred)).mean()\n",
    "\n",
    "mse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Model Tuning:\n",
    "\n",
    "#Setting parameters to tune\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(5,25))\n",
    "p=[1,2]\n",
    "#Weights is going to stay uniform\n",
    "#Algorithim will stay auto\n",
    "#Metric is the default minkowski metric\n",
    "\n",
    "#Set params, define the model and tune\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "hp = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "knn2 = KNeighborsRegressor()\n",
    "\n",
    "#Use a Gridsearch to tune\n",
    "knn_tuned = GridSearchCV(knn2, hp, cv=10)\n",
    "\n",
    "#Fit the tuned model & view parameters\n",
    "best_model = knn_tuned.fit(x_train, y_train)\n",
    "best_model.best_estimator_.get_params()\n",
    "\n",
    "\n",
    "#Apply the tuned parameters\n",
    "#Train the model\n",
    "#Then predict on the validation data\n",
    "knn_tuned2 = KNeighborsRegressor(algorithm='auto', leaf_size = 1,\n",
    "                            metric = 'minkowski', n_neighbors = 14,\n",
    "                            p = 1, weights = 'uniform')\n",
    "\n",
    "knn_tuned2.fit(x_train, y_train)\n",
    "\n",
    "#Make predictions on the validation dataset\n",
    "y_pred_knn = knn_tuned2.predict(x_val)\n",
    "\n",
    "#View Results\n",
    "mse(y_val, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the validation data & predictions back to their true values\n",
    "#So we can compare actual & predicted values based on the true values\n",
    "#(train2-1)*(train_max-train_min)+train_min\n",
    "\n",
    "#Inverse Log\n",
    "y_val2 = pd.DataFrame(np.exp(y_val))\n",
    "y_pred_knn2 = pd.DataFrame(np.exp(y_pred_knn))\n",
    "\n",
    "#Inverse Normalization\n",
    "y_val2_actual = (y_val2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "y_pred_knn3 = (y_pred_knn2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "results = pd.concat([y_val2_actual, y_pred_knn3], axis = 1)\n",
    "\n",
    "#Look at the true values and the predicted values (actual)\n",
    "pd.set_option('display.max_rows', None)\n",
    "results\n",
    "\n",
    "#Verify the reverse transformations were correct.\n",
    "#validation[\"runs_per_game\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Model: Elastic Net\n",
    "#See summary for explainations of the tuning\n",
    "#We are going straight into the L1/L2 tuning for this model\n",
    "\n",
    "\n",
    "#Elastic Net Model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#Building the model\n",
    "#Define parameters to tune\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "l1_ratio = np.arange(0.0, 1.0, 0.01)\n",
    "alpha =alpha\n",
    "#Set the parameters\n",
    "hp = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
    "#Set model\n",
    "elastic_net = ElasticNet()\n",
    "#Set resampling\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3)\n",
    "\n",
    "#Set gridsearch as the tuning method\n",
    "elnet = GridSearchCV(elastic_net, hp, cv=cv, n_jobs=-1)\n",
    "\n",
    "#Tune/fit the model on the training data\n",
    "elnet_tuned = elnet.fit(x_train, y_train)\n",
    "\n",
    "#View the best parameters\n",
    "elnet_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the tuned parameters\n",
    "#Fit the model with tuned paramters\n",
    "#Elastic Net from sklearn already implemented coordinate descent\n",
    "best_model_elnet = ElasticNet(alpha = 0.0001, l1_ratio=0.0)\n",
    "best_model_elnet2 = best_model_elnet.fit(x_train, y_train)\n",
    "\n",
    "#Make predictions on the validation dataset\n",
    "y_pred_elnet = best_model_elnet2.predict(x_val)\n",
    "\n",
    "#View the results\n",
    "mse(y_val, y_pred_elnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the validation data & predictions back to their true values\n",
    "y_val2 = pd.DataFrame(np.exp(y_val))\n",
    "y_pred_elnet2 = pd.DataFrame(np.exp(y_pred_elnet))\n",
    "y_val2_actual = (y_val2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "y_pred_elnet3 = (y_pred_elnet2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "results = pd.concat([y_val2_actual, y_pred_elnet3], axis = 1)\n",
    "results\n",
    "\n",
    "#Look at the Coefficients\n",
    "print(best_model_elnet2.intercept_, best_model_elnet2.coef_)\n",
    "\n",
    "#Transform the output so we can interpret model\n",
    "#Transform so we can make graphs and tables\n",
    "features = pd.DataFrame(validation.columns[0:24])\n",
    "coefs = pd.DataFrame(best_model_elnet2.coef_)\n",
    "res = pd.concat([features, coefs], axis = 1)\n",
    "res.columns = [\"Feature\", \"Coefficient\"]\n",
    "intercept = best_model_elnet2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficicent table\n",
    "import tabulate\n",
    "from tabulate import tabulate\n",
    "res2 = res\n",
    "res2.loc[len(res2.index)] = [\"Intercept\",intercept]\n",
    "print(tabulate(res2, headers=res2.columns, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the coefficient graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(res[\"Feature\"],res[\"Coefficient\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Elastic Net Coefficients')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "#Need to make the data into the xgbmatrix objects\n",
    "\n",
    "#Training\n",
    "train_features = train2.loc[:,\"Age\":\"IBB\"]\n",
    "train_label = train2[\"runs_per_game\"]\n",
    "dtrain = xgb.DMatrix(data = train_features, label = train_label)\n",
    "#Testing\n",
    "test_features = test2.loc[:,\"Age\":\"IBB\"]\n",
    "test_label = test2[\"runs_per_game\"]\n",
    "dtest = xgb.DMatrix(data = test_features, label = test_label)\n",
    "#Validation\n",
    "val_features = validation2.loc[:,\"Age\":\"IBB\"]\n",
    "val_label = validation2[\"runs_per_game\"]\n",
    "dval = xgb.DMatrix(data = val_features, label = val_label)\n",
    "\n",
    "\n",
    "#Fitting a standard model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:linear\")\n",
    "xgb_model.fit(X=train_features, y = train_label)\n",
    "xgb_preds = xgb_model.predict(val_features)\n",
    "mse(val_label, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning the XGB model\n",
    "#Using Randomsearch for this tuning since Gridsearch was used in the previous 2 models\n",
    "\n",
    "#Set the tuning parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#Using random search to get an idea of where parameters need to be set\n",
    "#Then we will probably go back and run a more focuesed gridsearch\n",
    "\n",
    "#Set the model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "#Dont need to include L1 regularization because\n",
    "#1. There is not high dimensionalty\n",
    "#Define the parameters\n",
    "parameters = {\n",
    "    'max_depth': [*range(6, 12, 1)],\n",
    "    'learning_rate': np.linspace(0.01,1.0,11),\n",
    "    'gamma': np.linspace(0,1.0,11),\n",
    "    'colsample_bytree': np.linspace(0.5,1.0,6),\n",
    "    'lambda': np.linspace(0.5,1.0,6)\n",
    "}\n",
    "\n",
    "\n",
    "xgb_rand = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=parameters,\n",
    "    n_iter = 500,\n",
    "    scoring = 'r2',\n",
    "    cv = 10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#Tune the model on the training data:\n",
    "xgb_tuned = xgb_rand.fit(X=train_features, y = train_label)\n",
    "\n",
    "#View the tuned parameters\n",
    "xgb_tuned.best_params_\n",
    "xgb_tuned.set_params = xgb_tuned.best_params_\n",
    "\n",
    "#Predict with the tuned model:\n",
    "xgb_preds = xgb_tuned.predict(val_features)\n",
    "mse(val_label, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the final predictions on validation data\n",
    "y_val2 = pd.DataFrame(np.exp(y_val))\n",
    "y_pred_xbg2 = pd.DataFrame(np.exp(xgb_preds))\n",
    "\n",
    "y_val2_actual = (y_val2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "y_pred_xbg3 = (y_pred_xbg2-1)*(train_max[\"runs_per_game\"]-train_min[\"runs_per_game\"])+train_min[\"runs_per_game\"]\n",
    "results = pd.concat([y_val2_actual, y_pred_xbg3], axis = 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting a model in a format we can interpret better\n",
    "#(Same model, just differnt format)\n",
    "\n",
    "#Get the model in a format that we can use\n",
    "xgb_regr = xgb.train(params=xgb_tuned.best_params_, dtrain=dtrain)\n",
    "\n",
    "#Predict with the tuned model:\n",
    "xgb_preds = xgb_regr.predict(dval)\n",
    "mse(val_label, xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Interpretations:\n",
    "\n",
    "#Variable Importance\n",
    "varimp1 = xgb.plot_importance(xgb_regr)\n",
    "varimp1 = varimp1.figure\n",
    "varimp1.set_size_inches(8, 10)\n",
    "\n",
    "varimp2 = xgb.plot_importance(xgb_regr, importance_type=\"gain\")\n",
    "varimp2 = varimp2.figure\n",
    "varimp2.set_size_inches(8, 10)\n",
    "\n",
    "#SHAP Plots\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(xgb_regr)\n",
    "shap_values = explainer.shap_values(dtrain)\n",
    "shap.summary_plot(shap_values, train_features)\n",
    "\n",
    "#Shap Dependence Plots:\n",
    "for name in train_features.columns:\n",
    "    shap.dependence_plot(name, shap_values, train_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f72acaec3a6472467059a708583c10030ddcb5bac1c9c072a73ee5c527c8099c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
